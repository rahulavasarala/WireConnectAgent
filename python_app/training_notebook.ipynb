{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b064e4",
   "metadata": {},
   "source": [
    "### Step 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us import all the packages we need to run the CLEAN_RL + Mujoco + Opensai pipeline\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Sequence\n",
    "import struct\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tyro\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "#@title Import MuJoCo, MJX, and Brax\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "import os\n",
    "from ml_collections import config_dict\n",
    "\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import jax.tree_util as jtu\n",
    "import numpy as np\n",
    "from flax.training import orbax_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.base import State as PipelineState\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "import zmq\n",
    "from distrax import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50994adb",
   "metadata": {},
   "source": [
    "### Step 1: Create MJX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mj_model_path = \"/Users/rahulavasarala/Desktop/OpenSai/WireConnectAgent/models/scenes/rizon4sgripper.xml\"\n",
    "mj_model = mujoco.MjModel.from_xml_path(mj_model_path)\n",
    "mj_data = mujoco.MjData(mj_model)\n",
    "mjx_model = mjx.put_model(mj_model)\n",
    "mjx_data = mjx.put_data(mj_model, mj_data)\n",
    "\n",
    "#Lets define the sensor addresses\n",
    "force_sensor_id = mujoco.mj_name2id(mj_model, mujoco.mjtObj.mjOBJ_SENSOR, \"force_sensor\")\n",
    "force_sensor_adr = mj_model.sensor_adr[force_sensor_id]\n",
    "\n",
    "torque_sensor_id = mujoco.mj_name2id(mj_model, mujoco.mjtObj.mjOBJ_SENSOR, \"torque_sensor\")\n",
    "torque_sensor_adr = mj_model.sensor_adr[torque_sensor_id]\n",
    "\n",
    "site_id = mj_model.sensor_objid[force_sensor_id]\n",
    "male_mount_id = model.body(name=\"male_connector_mount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f404f",
   "metadata": {},
   "source": [
    "#### 1.1: Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b849c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 5\n",
    "sim_duration = 10000\n",
    "stacked_data_dim = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cdaeac",
   "metadata": {},
   "source": [
    "#### 1.2: Initialize ZMQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e01c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_ENDPOINT = \"ipc:///tmp/zmq_torque_server\"\n",
    "ctx = zmq.Context()\n",
    "socket = ctx.socket(zmq.REQ)\n",
    "socket.connect(SERVER_ENDPOINT)\n",
    "\n",
    "FEMALE_POS = jp.array([0.1, 0.1, 0.2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad2f89",
   "metadata": {},
   "source": [
    "### Step 2: Create a custom pipeline env, that is zmq/jax compatible\n",
    "\n",
    "This will be a custom environment, where a batched call to an opensai ZMQ server will be made, and joint torques will be retrieved and environments will be stepped in a jax parallel way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_joint_torques(stacked_data: jp.ndarray, zmq_socket, num_envs):\n",
    "    packed_data = struct.pack('f' * stacked_data_dim * num_envs, *stacked_data)\n",
    "\n",
    "    zmq_socket.send(packed_data)\n",
    "    reply = socket.recv()\n",
    "    joint_torques = jp.frombuffer(reply, dtype = np.float32) \n",
    "\n",
    "    joint_torques = joint_torques.reshape((num_envs, 7)) \n",
    "    return joint_torques\n",
    "\n",
    "def _reset_env(state: mjx.Data, init_orientation: jp.ndarray):\n",
    "    qpos_init = mjx_model.qpos0\n",
    "    qvel_init = jp.zeros(qpos_init.shape)\n",
    "\n",
    "    return state.replace(qpos = qpos_init, qvel = qvel_init)\n",
    "\n",
    "_jit_reset_env = jax.jit(jax.vmap(_reset_env))\n",
    "\n",
    "def _apply_joint_torques_and_step_env(state: mjx.Data, joint_torque: jp.ndarray):\n",
    "    ctrls = state.ctrl\n",
    "    ctrls = ctrls.at[:7].set(joint_torque)\n",
    "    state = state.replace(ctrl = ctrls)\n",
    "\n",
    "    new_state = mjx.step(mjx_model, state)\n",
    "\n",
    "    return new_state\n",
    "\n",
    "_jit_apply_joint_torques_and_step_env = jax.jit(jax.vmap(_apply_joint_torques_and_step_env))\n",
    "\n",
    "#Set the joint positions and the joint velocities to certain values\n",
    "def _apply_joint_positions(state: mjx.Data, joint_pos: jp.ndarray):\n",
    "    jpos = state.qpos\n",
    "    jpos = jpos.at[:7].set(joint_pos)\n",
    "    state = state.replace(qpos = jpos)\n",
    "    jvel = state.qvel\n",
    "    jvel = jvel.at[:7].set(jp.zeros((7,)))\n",
    "    state = state.replace(qvel = jvel)\n",
    "\n",
    "    return state\n",
    "\n",
    "_jit_apply_joint_positions = jax.jit(jax.vmap(_apply_joint_positions))\n",
    "#So this whole time, I will assume that in the starting position, the male connector is in the gripper's hands\n",
    "#There is no movement of the position of the gripper\n",
    "\n",
    "#code to sample a random quaternion\n",
    "def random_quaternion(key):\n",
    "    u1, u2, u3 = jax.random.uniform(key, (3,), minval=0.0, maxval=1.0)\n",
    "\n",
    "    q1 = jnp.sqrt(1 - u1) * jnp.sin(2 * jnp.pi * u2)\n",
    "    q2 = jnp.sqrt(1 - u1) * jnp.cos(2 * jnp.pi * u2)\n",
    "    q3 = jnp.sqrt(u1) * jnp.sin(2 * jnp.pi * u3)\n",
    "    q4 = jnp.sqrt(u1) * jnp.cos(2 * jnp.pi * u3)\n",
    "\n",
    "    return jnp.array([q4, q1, q2, q3])  # w, x, y, z\n",
    "\n",
    "_batched_random_quaternions = jax.jit(jax.vmap(random_quaternion))\n",
    "\n",
    "\n",
    "def quaternion_to_z_axis(quat):\n",
    "    w, x, y, z = quat\n",
    "    z_axis = jnp.array([\n",
    "        2 * (x * z + w * y),\n",
    "        2 * (y * z - w * x),\n",
    "        1 - 2 * (x**2 + y**2)\n",
    "    ])\n",
    "    return z_axis\n",
    "\n",
    "#batch friendly quaternion to rotation map\n",
    "def quat_to_rotmat(q):\n",
    "    x, y, z, w = q[..., 0], q[..., 1], q[..., 2], q[..., 3]\n",
    "\n",
    "    xx, yy, zz = x * x, y * y, z * z\n",
    "    xy, xz, yz = x * y, x * z, y * z\n",
    "    wx, wy, wz = w * x, w * y, w * z\n",
    "\n",
    "    R = jp.stack([\n",
    "        jp.stack([1 - 2 * (yy + zz),     2 * (xy - wz),         2 * (xz + wy)], axis=-1),\n",
    "        jp.stack([    2 * (xy + wz), 1 - 2 * (xx + zz),         2 * (yz - wx)], axis=-1),\n",
    "        jp.stack([    2 * (xz - wy),     2 * (yz + wx),     1 - 2 * (xx + yy)], axis=-1),\n",
    "    ], axis=-2)\n",
    "\n",
    "    return R \n",
    "\n",
    "def find_quick_reset_desired_positions(female_orientation: jp.ndarray, female_translation: jp.ndarray): #I will need to test this logic in the c++ build\n",
    "    des_cartesian_pos = FEMALE_POS + female_translation + 0.2*quaternion_to_z_axis(female_orientation)\n",
    "\n",
    "    return des_cartesian_pos\n",
    "\n",
    "_batched_find_quick_reset_desired_positions = jax.jit(jax.vmap(find_quick_reset_desired_positions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597fb967",
   "metadata": {},
   "source": [
    "#### Lets create code to initialize the pointclouds(in the percieved frame of the female connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48f112",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3986320548.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def create_point_cloud(fem_stl: str):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "NUM_POINTS = 50\n",
    "FEMALE_CONNECTOR_GLOBAL_POS = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "def create_point_cloud(fem_stl: str):\n",
    "\n",
    "    mesh = o3d.io.read_triangle_mesh(fem_stl)\n",
    "    mesh.compute_vertex_normals()\n",
    "    pcd = mesh.sample_points_poisson_disk(number_of_points=NUM_POINTS)\n",
    "\n",
    "    return pcd\n",
    "\n",
    "def create_random_trans_orients(key, num_envs, r_std = 0.01, t_std = 0.01):\n",
    "\n",
    "    orients = jp.zeros((num_envs, 3))\n",
    "    translations = jp.zeros((num_envs, 3))\n",
    "\n",
    "    for i in range(num_envs):\n",
    "        orients[i] =  jax.random.normal(key, shape=(3,)) * r_std\n",
    "        translations[i] = FEMALE_CONNECTOR_GLOBAL_POS + jax.random.normal(key, shape=(3,)) * t_std\n",
    "\n",
    "    return translations, orients\n",
    "\n",
    "def create_noisy_point_clouds(noise_std: float, trans: jp.ndarray, orients: jp.ndarray, base_pcd):\n",
    "\n",
    "    noisy_pcds = jp.zeros((num_envs, NUM_POINTS, 3))\n",
    "\n",
    "    for i in range(num_envs):\n",
    "\n",
    "        pcd = base_pcd.copy()\n",
    "        # rotation_matrix = R.from_euler('xyz', orients[i]).as_matrix()\n",
    "        # pcd.rotate(rotation_matrix, center=(0,0,0))\n",
    "        pcd.translate(FEMALE_CONNECTOR_GLOBAL_POS + np.array(trans[i]))\n",
    "\n",
    "        np_points = np.asarray(pcd.points)\n",
    "        jp_points = jp.array(np_points)\n",
    "\n",
    "        noisy_pcds[i] = jp_points\n",
    "\n",
    "    return noisy_pcds\n",
    "\n",
    "#This is the get single observation function ***Female refers to the frame of the female connector\n",
    "def get_single_obs(state: mjx.Data, female_orient: jp.ndarray, female_translation: jp.ndarray, points: jp.ndarray, no_contact_count: int):\n",
    "\n",
    "    #Lets get the force/torque data in the world frame, and then put it in the orient frame\n",
    "    force = state.sensordata[force_sensor_adr : force_sensor_adr + 3]\n",
    "    torque = state.sensordata[torque_sensor_adr: torque_sensor_adr + 3]\n",
    "\n",
    "    #transform to world frame\n",
    "    R_flat = state.xmat[site_id]\n",
    "    R_world_sensor = R_flat.reshape((3,3))\n",
    "\n",
    "    force_world = R_world_sensor @ force\n",
    "    torque_world = R_world_sensor @ torque\n",
    "\n",
    "    #transform to the orientation frame, orient -> R -> R^T\n",
    "    female_body_id = mujoco.mj_name2id(mj_model, mujoco.mjtObj.mjOBJ_BODY, \"female_connector_truncated\")\n",
    "    R_world_female = jp.reshape(mj_data.xmat[male_body_id], (3, 3)) @ quat_to_rotmat(female_orient)\n",
    "    R_female_world = R_world_female.T\n",
    "\n",
    "    force_female = R_female_world* force_world\n",
    "    torque_female = R_female_world * torque_world\n",
    "\n",
    "    xmat = state.xmat[male_mount_id]   \n",
    "    R_world_male = xmat.reshape(3, 3)  \n",
    "\n",
    "    male_orientation_female = R_female_world * R_world_male\n",
    "    male_pos_world = state.xpos[male_mount_id]\n",
    "    male_pos_female = R_female_world @ (male_pos_world - female_translation + FEMALE_CONNECTOR_GLOBAL_POS) #This is using the first translation then rotation strategy\n",
    "\n",
    "    obs = jp.concatenate([force_female, torque_female, male_pos_female, male_orientation_female, points.flatten()])\n",
    "\n",
    "    #Get the key points in the world frame for male connector(in world frame because distance does not matter) \n",
    "    key_points_male = jp.array([[0.007, 0.007, -0.007, -0.007],\n",
    "                                [0.0, 0.0 , 0.0, 0.0], \n",
    "                                [0.0, 0.01, 0.01, 0.0]])\n",
    "    \n",
    "    male_body_id = mujoco.mj_name2id(mj_model, mujoco.mjtObj.mjOBJ_BODY, \"male-connector-minimal\")\n",
    "    mcm_pos = jp.array(mj_data.xpos[male_body_id]) \n",
    "    R_world_male = jp.reshape(mj_data.xmat[male_body_id], (3, 3))\n",
    "    world_key_points_male = R_world_male @ key_points_male + mcm_pos[:, None]\n",
    "\n",
    "    #Get key points for female connector(in world frame)\n",
    "\n",
    "    key_points_female = jp.array([[-0.007, -0.007, 0.007, 0.007],\n",
    "                                [0.0, 0.008 , 0.008, 0.0], \n",
    "                                [0.0, 0.0, 0.0, 0.0]])\n",
    "\n",
    "    fct_pos = jp.array(mj_data.xpos[female_body_id]) + female_translation\n",
    "    world_key_points_female = R_world_female @ key_points_female + fct_pos[:, None]\n",
    "\n",
    "    reward = jp.linalg.norm(world_key_points_female - world_key_points_male)\n",
    "\n",
    "    done = False\n",
    "    #The no contact count update logic will happen over here: \n",
    "    if jp.linalg.norm(force_world) > 0.01:\n",
    "        no_contact_count = 0\n",
    "    else:\n",
    "        no_contact_count += 1\n",
    "        if no_contact_count > 15: #Threshold to be set\n",
    "            done = True\n",
    "\n",
    "    return obs, reward, done, no_contact_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c582de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomZMQEnvironment:\n",
    "    def __init__(self, zmq_socket, n_physics_steps, init_des_cart_pos = jp.zeros((3,)), num_envs = 5, action_space_dim = 7, observation_space_dim = 10):\n",
    "        self.num_envs = num_envs\n",
    "        self.env_ids = jp.arange(num_envs)\n",
    "\n",
    "        self.zmq_socket = zmq_socket\n",
    "\n",
    "        self.n_frames = n_physics_steps\n",
    "        self.female_orientations = jp.zeros((self.num_envs, 4))\n",
    "        self.female_translations = jp.zeros((self.num_envs, 3))\n",
    "        self.female_pos = FEMALE_POS\n",
    "\n",
    "        self.des_cart_pos = jp.tile(init_des_cart_pos, (self.num_envs, 1))\n",
    "        self.des_cart_orient = jp.tile(jp.array([1,0,0,0]), (self.num_envs, 1))\n",
    "\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self.observation_space_dim = observation_space_dim\n",
    "\n",
    "        self.quick_reset_positions = jp.zeros((self.num_envs, 7))\n",
    "        self.quick_reset_status = False\n",
    "\n",
    "        self.no_contact_count = jp.zeros((num_envs,))\n",
    "\n",
    "    def generate_random_orientations_female_connector(self):\n",
    "\n",
    "        key = jax.random.PRNGKey(42)\n",
    "        self.female_orientations, self.female_translations = create_random_trans_orients(key, self.num_envs)\n",
    "\n",
    "\n",
    "    def reset(self, states):\n",
    "        states = _jit_reset_env(states, self.init_orientations) #v_map'd\n",
    "\n",
    "        if not self.quick_reset_status:\n",
    "            self.des_cart_pos = _batched_find_quick_reset_desired_positions(self.female_orientations, self.female_translations) #The logic for this should include the translation as well\n",
    "            self.des_cart_orient = jp.concatenate([self.female_orientations[:, :1], -self.female_orientations[:, 1:]], axis=1) #need to test out the reset functionality\n",
    "\n",
    "            for i in range(4000):\n",
    "                joint_torques = self.get_joint_torques(states)\n",
    "                states = self.apply_joint_torques_and_step(states, joint_torques)\n",
    "\n",
    "            self.quick_reset_positions = jax.vmap(lambda d: d.qpos)(states)\n",
    "            self.quick_reset_status = True\n",
    "        else:\n",
    "            states  = _jit_apply_joint_positions(states, self.quick_reset_positions)\n",
    "\n",
    "        obs = jp.zeros((self.num_envs, self.observation_space_dim))\n",
    "\n",
    "        return states, obs\n",
    "    \n",
    "    def quick_reset(self, states):\n",
    "        #reset to the quick reset joint positions\n",
    "\n",
    "        if not self.quick_reset_status:\n",
    "            return states, self._get_obs()\n",
    "        \n",
    "        states = _jit_apply_joint_positions(states, self.quick_reset_positions)\n",
    "\n",
    "        return states\n",
    "    \n",
    "    def get_joint_torques(self, states) -> jp.ndarray:\n",
    "\n",
    "        qpos_batch = jax.vmap(lambda d: d.qpos)(states) #we can jit these\n",
    "        qvel_batch = jax.vmap(lambda d: d.qvel)(states)\n",
    "\n",
    "        des_targets = jp.concatenate([self.des_cart_pos, self.des_cart_orient], axis = 1)\n",
    "\n",
    "        stacked_data = jp.concatenate([des_targets.flatten(), qpos_batch.flatten(), qvel_batch.flatten()])\n",
    "        joint_torques = fetch_joint_torques(stacked_data, socket, self.num_envs)\n",
    "\n",
    "        return joint_torques\n",
    "\n",
    "    def apply_joint_torques_and_step(self, states, joint_torques: jp.ndarray):\n",
    "        states = _jit_apply_joint_torques_and_step_env(states, joint_torques) #vmap'd\n",
    "        return states\n",
    "\n",
    "    def step(self, states, actions: jp.ndarray):\n",
    "\n",
    "        for _ in range(self.n_frames -1):\n",
    "            joint_torques = self.get_joint_torques(states)\n",
    "            states = self.apply_joint_torques_and_step(states, joint_torques)\n",
    "\n",
    "        ## Apply the action logic:\n",
    "\n",
    "        joint_torques = self.get_joint_torques(states)\n",
    "        states = self.apply_joint_torques_and_step(states, joint_torques)\n",
    "\n",
    "        ## Return the observation data\n",
    "\n",
    "        return states, self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "\n",
    "        rewards = jp.zeros(self.num_envs)\n",
    "        dones = jp.zeros(self.num_envs)\n",
    "        obs = jp.zeros((self.num_envs, self.observation_space_dim))\n",
    "        info = {\n",
    "            \"reward\": rewards,\n",
    "            \"terminated\": dones,\n",
    "            \"TimeLimit.truncated\": jp.zeros_like(dones)\n",
    "        }\n",
    "\n",
    "        return (obs, rewards, dones, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db536a80",
   "metadata": {},
   "source": [
    "### Training Loop And Things of that nature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b838447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix weird OOM https://github.com/google/jax/discussions/6332#discussioncomment-1279991\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.6\"\n",
    "# Fix CUDNN non-determinisim; https://github.com/google/jax/issues/4823#issuecomment-952835771\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--xla_gpu_autotune_level=2 --xla_gpu_deterministic_reductions\"\n",
    "os.environ[\"TF_CUDNN DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = os.path.basename(__file__)[: -len(\".py\")]\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"Breakout-v5\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 10000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 5\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 4\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 4\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.1\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b389a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    hidden_sizes: Sequence[int] = (256, 256)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for size in self.hidden_sizes:\n",
    "            x = nn.Dense(size, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "            x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return nn.Dense(1, kernel_init=orthogonal(1), bias_init=constant(0.0))(x)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    action_dim: int\n",
    "    log_std_init: float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        mean = nn.Dense(self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(x)\n",
    "        log_std = self.param(\"log_std\", lambda key, shape: jnp.full(shape, self.log_std_init), (self.action_dim,))\n",
    "        std = jnp.exp(log_std)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class AgentParams:\n",
    "    network_params: flax.core.FrozenDict\n",
    "    actor_params: flax.core.FrozenDict\n",
    "    critic_params: flax.core.FrozenDict\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class Storage:\n",
    "    obs: jnp.array\n",
    "    actions: jnp.array\n",
    "    logprobs: jnp.array\n",
    "    dones: jnp.array\n",
    "    values: jnp.array\n",
    "    advantages: jnp.array\n",
    "    returns: jnp.array\n",
    "    rewards: jnp.array\n",
    "\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class EpisodeStatistics:\n",
    "    episode_returns: jnp.array\n",
    "    episode_lengths: jnp.array\n",
    "    returned_episode_returns: jnp.array\n",
    "    returned_episode_lengths: jnp.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe520ba",
   "metadata": {},
   "source": [
    "### Actual Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = tyro.cli(Args)\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "args.num_iterations = args.total_timesteps // args.batch_size\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "key, network_key, actor_key, critic_key = jax.random.split(key, 4)\n",
    "\n",
    "# env setup\n",
    "init_states = [mjx_data.replace() for _ in range(args.num_envs)]\n",
    "states = jtu.tree_map(lambda *xs: jp.stack(xs), *init_states)\n",
    "mjEnvs = CustomZMQEnvironment(socket, n_physics_steps=5,\n",
    "                                init_des_cart_pos=jp.array([0.1,0.1,0.1]), num_envs=args.num_envs)\n",
    "\n",
    "episode_stats = EpisodeStatistics(\n",
    "    episode_returns=jnp.zeros(args.num_envs, dtype=jnp.float32),\n",
    "    episode_lengths=jnp.zeros(args.num_envs, dtype=jnp.int32),\n",
    "    returned_episode_returns=jnp.zeros(args.num_envs, dtype=jnp.float32),\n",
    "    returned_episode_lengths=jnp.zeros(args.num_envs, dtype=jnp.int32),\n",
    ")\n",
    "\n",
    "handle = states\n",
    "\n",
    "def step_env_wrappeed(episode_stats, handle, action):\n",
    "    handle, (next_obs, reward, next_done, info) = mjEnvs.step(handle, action)\n",
    "    new_episode_return = episode_stats.episode_returns + info[\"reward\"]\n",
    "    new_episode_length = episode_stats.episode_lengths + 1\n",
    "    episode_stats = episode_stats.replace(\n",
    "        episode_returns=(new_episode_return) * (1 - info[\"terminated\"]) * (1 - info[\"TimeLimit.truncated\"]),\n",
    "        episode_lengths=(new_episode_length) * (1 - info[\"terminated\"]) * (1 - info[\"TimeLimit.truncated\"]),\n",
    "        # only update the `returned_episode_returns` if the episode is done\n",
    "        returned_episode_returns=jnp.where(\n",
    "            info[\"terminated\"] + info[\"TimeLimit.truncated\"], new_episode_return, episode_stats.returned_episode_returns\n",
    "        ),\n",
    "        returned_episode_lengths=jnp.where(\n",
    "            info[\"terminated\"] + info[\"TimeLimit.truncated\"], new_episode_length, episode_stats.returned_episode_lengths\n",
    "        ),\n",
    "    )\n",
    "    return episode_stats, handle, (next_obs, reward, next_done, info)\n",
    "\n",
    "# assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "def linear_schedule(count):\n",
    "    # anneal learning rate linearly after one training iteration which contains\n",
    "    # (args.num_minibatches * args.update_epochs) gradient updates\n",
    "    frac = 1.0 - (count // (args.num_minibatches * args.update_epochs)) / args.num_iterations\n",
    "    return args.learning_rate * frac\n",
    "\n",
    "network = Network()\n",
    "actor = Actor(action_dim=mjEnvs.action_space_dim)\n",
    "critic = Critic()\n",
    "network_params = network.init(network_key, np.zeros((1, mjEnvs.observation_space_dim)))\n",
    "\n",
    "agent_params = FrozenDict({\n",
    "    \"network\": network_params,\n",
    "    \"actor\": actor.init(actor_key, network.apply(network_params, np.zeros((1, mjEnvs.observation_space_dim)))),\n",
    "    \"critic\": critic.init(critic_key, network.apply(network_params, np.zeros((1, mjEnvs.observation_space_dim)))),\n",
    "})\n",
    "\n",
    "agent_state = TrainState.create(\n",
    "    apply_fn=None,\n",
    "    params=agent_params,\n",
    "    tx=optax.chain(\n",
    "        optax.clip_by_global_norm(args.max_grad_norm),\n",
    "        optax.inject_hyperparams(optax.adam)(\n",
    "            learning_rate=linear_schedule if args.anneal_lr else args.learning_rate, eps=1e-5\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "network.apply = jax.jit(network.apply)\n",
    "actor.apply = jax.jit(actor.apply)\n",
    "critic.apply = jax.jit(critic.apply)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "storage = Storage(\n",
    "    obs=jnp.zeros((args.num_steps, args.num_envs, mjEnvs.observation_space_dim)),\n",
    "    actions=jnp.zeros((args.num_steps, args.num_envs, mjEnvs.action_space_dim)),\n",
    "    logprobs=jnp.zeros((args.num_steps, args.num_envs)),\n",
    "    dones=jnp.zeros((args.num_steps, args.num_envs)),\n",
    "    values=jnp.zeros((args.num_steps, args.num_envs)),\n",
    "    advantages=jnp.zeros((args.num_steps, args.num_envs)),\n",
    "    returns=jnp.zeros((args.num_steps, args.num_envs)),\n",
    "    rewards=jnp.zeros((args.num_steps, args.num_envs)),\n",
    ")\n",
    "\n",
    "#need to make this compatible with continuous distributions\n",
    "@jax.jit\n",
    "def get_action_and_value(\n",
    "    agent_state: TrainState,\n",
    "    next_obs: np.ndarray,\n",
    "    next_done: np.ndarray,\n",
    "    storage: Storage,\n",
    "    step: int,\n",
    "    key: jax.random.PRNGKey,\n",
    "):\n",
    "    \"\"\"sample action, calculate value, logprob, entropy, and update storage\"\"\"\n",
    "    hidden = network.apply(agent_state.params[\"network\"], next_obs)\n",
    "    mean, std = actor.apply(agent_state.params[\"actor\"], hidden)\n",
    "\n",
    "    #sample action from distribution(continuous action space)\n",
    "    dist = Normal(loc=mean, scale=std)\n",
    "    \n",
    "    key, subkey = jax.random.split(key)\n",
    "    action = dist.sample(seed=subkey)\n",
    "    logprob = dist.log_prob(action).sum(-1) \n",
    "\n",
    "    value = critic.apply(agent_state.params[\"critic\"], hidden)\n",
    "    storage = storage.replace(\n",
    "        obs=storage.obs.at[step].set(next_obs),\n",
    "        dones=storage.dones.at[step].set(next_done),\n",
    "        actions=storage.actions.at[step].set(action),\n",
    "        logprobs=storage.logprobs.at[step].set(logprob),\n",
    "        values=storage.values.at[step].set(value.squeeze()),\n",
    "    )\n",
    "    return storage, action, key\n",
    "\n",
    "@jax.jit\n",
    "def get_action_and_value2(\n",
    "    params: flax.core.FrozenDict,\n",
    "    x: np.ndarray,\n",
    "    action: np.ndarray,\n",
    "):\n",
    "    \"\"\"calculate value, logprob of supplied `action`, and entropy\"\"\"\n",
    "    hidden = network.apply(params[\"network\"], x)\n",
    "    mean, std = actor.apply(params[\"actor\"], hidden)\n",
    "\n",
    "    dist = Normal(loc=mean, scale=std)\n",
    "    logprob = dist.log_prob(action).sum(-1) \n",
    "\n",
    "    entropy = dist.entropy().sum(-1)\n",
    "    \n",
    "    value = critic.apply(params[\"critic\"], hidden).squeeze()\n",
    "    return logprob, entropy, value\n",
    "\n",
    "@jax.jit\n",
    "def compute_gae(\n",
    "    agent_state: TrainState,\n",
    "    next_obs: np.ndarray,\n",
    "    next_done: np.ndarray,\n",
    "    storage: Storage,\n",
    "):\n",
    "    storage = storage.replace(advantages=storage.advantages.at[:].set(0.0))\n",
    "    next_value = critic.apply(\n",
    "        agent_state.params[\"critic\"], network.apply(agent_state.params[\"network\"], next_obs)\n",
    "    ).squeeze()\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(args.num_steps)):\n",
    "        if t == args.num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - storage.dones[t + 1]\n",
    "            nextvalues = storage.values[t + 1]\n",
    "        delta = storage.rewards[t] + args.gamma * nextvalues * nextnonterminal - storage.values[t]\n",
    "        lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        storage = storage.replace(advantages=storage.advantages.at[t].set(lastgaelam))\n",
    "    storage = storage.replace(returns=storage.advantages + storage.values)\n",
    "    return storage\n",
    "\n",
    "@jax.jit\n",
    "def update_ppo(\n",
    "    agent_state: TrainState,\n",
    "    storage: Storage,\n",
    "    key: jax.random.PRNGKey,\n",
    "):\n",
    "    b_obs = storage.obs.reshape((-1, mjEnvs.observation_space_dim)) \n",
    "    b_logprobs = storage.logprobs.reshape(-1)\n",
    "    b_actions = storage.actions.reshape(-1, mjEnvs.action_space_dim)\n",
    "    b_advantages = storage.advantages.reshape(-1)\n",
    "    b_returns = storage.returns.reshape(-1)\n",
    "\n",
    "    def ppo_loss(params, x, a, logp, mb_advantages, mb_returns):\n",
    "        newlogprob, entropy, newvalue = get_action_and_value2(params, x, a)\n",
    "        logratio = newlogprob - logp\n",
    "        ratio = jnp.exp(logratio)\n",
    "        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "\n",
    "        if args.norm_adv:\n",
    "            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "        # Policy loss\n",
    "        pg_loss1 = -mb_advantages * ratio\n",
    "        pg_loss2 = -mb_advantages * jnp.clip(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "        pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "        # Value loss\n",
    "        v_loss = 0.5 * ((newvalue - mb_returns) ** 2).mean()\n",
    "\n",
    "        entropy_loss = entropy.mean()\n",
    "        loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "        return loss, (pg_loss, v_loss, entropy_loss, jax.lax.stop_gradient(approx_kl))\n",
    "\n",
    "    ppo_loss_grad_fn = jax.value_and_grad(ppo_loss, has_aux=True)\n",
    "    for _ in range(args.update_epochs):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        b_inds = jax.random.permutation(subkey, args.batch_size, independent=True)\n",
    "        for start in range(0, args.batch_size, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "            (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(\n",
    "                agent_state.params,\n",
    "                b_obs[mb_inds],\n",
    "                b_actions[mb_inds],\n",
    "                b_logprobs[mb_inds],\n",
    "                b_advantages[mb_inds],\n",
    "                b_returns[mb_inds],\n",
    "            )\n",
    "            agent_state = agent_state.apply_gradients(grads=grads)\n",
    "    return agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, key\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "handle, next_obs = mjEnvs.reset(handle)\n",
    "next_done = np.zeros(args.num_envs)\n",
    "\n",
    "def rollout(agent_state, episode_stats, next_obs, next_done, storage, key, handle, global_step):\n",
    "    for step in range(0, args.num_steps):\n",
    "        global_step += args.num_envs\n",
    "        storage, action, key = get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        episode_stats, handle, (next_obs, reward, next_done, _) = step_env_wrappeed(episode_stats, handle, action)\n",
    "        storage = storage.replace(rewards=storage.rewards.at[step].set(reward))\n",
    "    return agent_state, episode_stats, next_obs, next_done, storage, key, handle, global_step\n",
    "\n",
    "for iteration in range(1, args.num_iterations + 1):\n",
    "    iteration_time_start = time.time()\n",
    "    agent_state, episode_stats, next_obs, next_done, storage, key, handle, global_step = rollout(\n",
    "        agent_state, episode_stats, next_obs, next_done, storage, key, handle, global_step\n",
    "    )\n",
    "    storage = compute_gae(agent_state, next_obs, next_done, storage)\n",
    "    agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, key = update_ppo(\n",
    "        agent_state,\n",
    "        storage,\n",
    "        key,\n",
    "    )\n",
    "    avg_episodic_return = np.mean(jax.device_get(episode_stats.returned_episode_returns))\n",
    "    print(f\"global_step={global_step}, avg_episodic_return={avg_episodic_return}\")\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/avg_episodic_return\", avg_episodic_return, global_step)\n",
    "    writer.add_scalar(\n",
    "        \"charts/avg_episodic_length\", np.mean(jax.device_get(episode_stats.returned_episode_lengths)), global_step\n",
    "    )\n",
    "    writer.add_scalar(\"charts/learning_rate\", agent_state.opt_state[1].hyperparams[\"learning_rate\"].item(), global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/loss\", loss.item(), global_step)\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    writer.add_scalar(\n",
    "        \"charts/SPS_update\", int(args.num_envs * args.num_steps / (time.time() - iteration_time_start)), global_step\n",
    "    )\n",
    "\n",
    "del mjEnvs\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
